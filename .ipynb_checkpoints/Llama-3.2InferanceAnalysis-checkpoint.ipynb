{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "616f0ed9-9ebc-445c-946a-5d2da1bf1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b0cd07d67648118eb37de61b0eb3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model & Processor Loaded!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of image token (0) should be the same as in the number of provided images (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n\u001b[1;32m     27\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image>What is the famous thing in the image?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, text\u001b[38;5;241m=\u001b[39mprompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/mllama/processing_mllama.py:308\u001b[0m, in \u001b[0;36mMllamaProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo image were provided, but there are image tokens in the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of image token (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(n_images_in_text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) should be the same as in the number of provided images (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(n_images_in_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m             )\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimages_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of image token (0) should be the same as in the number of provided images (1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load processor and model\n",
    "model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"eager\",\n",
    "    output_attentions=True,\n",
    "    return_dict=True\n",
    ").to(\"cpu\")\n",
    "\n",
    "print(\"✅ Model & Processor Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb57d04c-44b3-47b1-858c-eb13cf6a8519",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of image token (0) should be the same as in the number of provided images (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# prompt = \"<image> What is the famous thing in the image?\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template([\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_generation_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho invented electricity?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     10\u001b[0m ], tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39m[image], text\u001b[38;5;241m=\u001b[39mprompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/mllama/processing_mllama.py:308\u001b[0m, in \u001b[0;36mMllamaProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo image were provided, but there are image tokens in the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of image token (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(n_images_in_text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) should be the same as in the number of provided images (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(n_images_in_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m             )\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimages_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of image token (0) should be the same as in the number of provided images (1)"
     ]
    }
   ],
   "source": [
    "# Load image\n",
    "image_path = \"image.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Prepare inputs\n",
    "# prompt = \"<image> What is the famous thing in the image?\"\n",
    "# prompt = processor.tokenizer.apply_chat_template([\n",
    "#     {\"role\": \"user\", \"content\": \"<image>\", \"add_generation_prompt\": False},\n",
    "#     {\"role\": \"user\", \"content\": \"Who invented electricity?\"}\n",
    "# ], tokenize=False)\n",
    " messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is the famous thing in the image?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# ✅ Tokenize text prompt\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(images=[image], text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# Decode output\n",
    "generated_texts = processor.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "print(\"📝 Generated Answer:\", generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b77ad-dafd-499d-b02a-7580e33d5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_layers = [3, 8, 13, 18, 23, 28, 33, 38]\n",
    "\n",
    "# Get token strings\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "# Create directory to store maps\n",
    "output_dir = \"./cross_attention_maps\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize cross-attention\n",
    "for layer in cross_attn_layers:\n",
    "    cross_attn = outputs.attentions[layer][0]  # shape: [num_heads, text_seq_len, image_seq_len]\n",
    "    attn_mean = cross_attn.mean(dim=0)         # [text_seq_len, image_seq_len]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(attn_mean.numpy(), cmap=\"YlGnBu\", annot=False)\n",
    "    plt.title(f\"Cross-Attention: Text Tokens → Image Embeddings (Layer {layer})\")\n",
    "    plt.xlabel(\"Image Patches\")\n",
    "    plt.ylabel(\"Text Tokens\")\n",
    "    plt.yticks(ticks=range(len(tokens)), labels=tokens, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/cross_attention_layer_{layer}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ea30f-8289-47e1-9e2a-b4b0e11ccdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d079a-97a8-42bc-99d3-8b44da7acc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a422d6-ab26-4db1-9a51-d4512b815b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
