{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616f0ed9-9ebc-445c-946a-5d2da1bf1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233e8822b0c4452a8192cdaac9cbf743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model & Processor Loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load processor and model\n",
    "model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"eager\",\n",
    "    output_attentions=True,\n",
    "    return_dict=True\n",
    ").to(\"cpu\")\n",
    "\n",
    "print(\"‚úÖ Model & Processor Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb57d04c-44b3-47b1-858c-eb13cf6a8519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated Answer: QuestionQuestion\n",
      "\n",
      "##The is the symbol statue on the picture? Theassistant\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Load image\n",
    "image_path = \"image.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Prepare inputs\n",
    "# prompt = \"<image> What is the famous thing in the image?\"\n",
    "# prompt = processor.tokenizer.apply_chat_template([\n",
    "#     {\"role\": \"user\", \"content\": \"<image>\", \"add_generation_prompt\": False},\n",
    "#     {\"role\": \"user\", \"content\": \"Who invented electricity?\"}\n",
    "# ], tokenize=False)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is the famous thing in the image?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# ‚úÖ Tokenize text prompt\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(images=[image], text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# Decode output\n",
    "generated_texts = processor.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "print(\"üìù Generated Answer:\", generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9b77ad-dafd-499d-b02a-7580e33d5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_layers = [3, 8, 13, 18, 23, 28, 33, 38]\n",
    "\n",
    "# Get token strings\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "# Create directory to store maps\n",
    "output_dir = \"./cross_attention_maps\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize cross-attention\n",
    "for layer in cross_attn_layers:\n",
    "    cross_attn = outputs.attentions[layer][0]  # shape: [num_heads, text_seq_len, image_seq_len]\n",
    "    attn_mean = cross_attn.mean(dim=0)         # [text_seq_len, image_seq_len]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(attn_mean.numpy(), cmap=\"YlGnBu\", annot=False)\n",
    "    plt.title(f\"Cross-Attention: Text Tokens ‚Üí Image Embeddings (Layer {layer})\")\n",
    "    plt.xlabel(\"Image Patches\")\n",
    "    plt.ylabel(\"Text Tokens\")\n",
    "    plt.yticks(ticks=range(len(tokens)), labels=tokens, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/cross_attention_layer_{layer}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ea30f-8289-47e1-9e2a-b4b0e11ccdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f97d079a-97a8-42bc-99d3-8b44da7acc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patch_202.png',\n",
       " 'patch_216.png',\n",
       " 'patch_028.png',\n",
       " 'patch_000.png',\n",
       " 'patch_014.png',\n",
       " 'patch_148.png',\n",
       " 'patch_174.png',\n",
       " 'patch_160.png',\n",
       " 'patch_161.png',\n",
       " 'patch_175.png']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Create directory to save image patches\n",
    "patch_output_dir = \"./image_patches_14x14\"\n",
    "os.makedirs(patch_output_dir, exist_ok=True)\n",
    "\n",
    "# Reload the image\n",
    "image_path = \"image.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\").resize((224, 224))\n",
    "\n",
    "# Patch parameters\n",
    "patch_size = 14\n",
    "patch_count = 0\n",
    "\n",
    "# Split and save patches\n",
    "for row in range(0, 224, patch_size):\n",
    "    for col in range(0, 224, patch_size):\n",
    "        patch = image.crop((col, row, col + patch_size, row + patch_size))\n",
    "        patch.save(os.path.join(patch_output_dir, f\"patch_{patch_count:03d}.png\"))\n",
    "        patch_count += 1\n",
    "\n",
    "import os\n",
    "os.listdir(patch_output_dir)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a422d6-ab26-4db1-9a51-d4512b815b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
