{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f73c9e01-c0dd-455b-8ca2-595c42366bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(57672) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0.dev20241112)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (0.29.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1843e810-bd07-4e5e-b22a-132c3a40d5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n",
      "⏳ Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686cf73f92fc48de85efc224cb1655e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "✅ Processor loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# ✅ Detect Apple Metal GPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ✅ Load Model & Processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "print(\"⏳ Loading model...\")\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "print(\"✅ Processor loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47246310-2e27-4236-9053-4ee62f978142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded image: ./image.jpg, Size: (960, 504)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(72177) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(72180) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# ✅ Load image from local path\n",
    "image_path = \"./image.jpg\"  # CHANGE THIS to your actual image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "print(f\"✅ Loaded image: {image_path}, Size: {image.size}\")\n",
    "image.show()  # Show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8062177c-be04-4d0c-bb2f-82a19d69a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed input text for the model!\n",
      "✅ Inputs prepared, ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Create Question Prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Describe the person's face on the left?\"}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "print(\"✅ Processed input text for the model!\")\n",
    "\n",
    "# ✅ Tokenize Inputs\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "print(\"✅ Inputs prepared, ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563e4ca-a269-4a86-9e74-d625ab2255bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MllamaModel is using MllamaTextSelfSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "MllamaModel is using MllamaTextCrossSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Run Inference with Attention Extraction\n",
    "print(\"⏳ Running inference...\")\n",
    "model.config.output_attentions = True\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        output_attentions=True,  # Ensure attention extraction\n",
    "        return_dict_in_generate=True  # Return a dictionary for easy extraction\n",
    "    )\n",
    "\n",
    "# ✅ Extract attention maps correctly\n",
    "if \"attentions\" in outputs:\n",
    "    attentions = outputs.attentions  # Extract all decoder layer attentions\n",
    "    print(f\"✅ Inference completed! Extracted {len(attentions)} attention layers.\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Attention extraction failed. The model did not return attention weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1d93e-7b04-4165-ad4a-015ab9126476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aeb273-8552-457e-b059-793167bacd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# ✅ Iterate through each decoder layer's attention\n",
    "for layer_idx, layer_attn in enumerate(attentions):\n",
    "    print(f\"Processing attention map for Layer {layer_idx + 1}\")\n",
    "\n",
    "    if isinstance(layer_attn, tuple):  \n",
    "        # ✅ Extract the first element if it's a tuple\n",
    "        layer_attn = layer_attn[0]\n",
    "\n",
    "    # ✅ Ensure the attention map has the correct dimensions\n",
    "    if layer_attn.ndim == 4:  # Expected (batch_size, num_heads, seq_len, seq_len)\n",
    "        # ✅ Average across all attention heads\n",
    "        attn_map = layer_attn.mean(dim=1).squeeze().cpu().detach().numpy()\n",
    "    else:\n",
    "        print(f\"⚠️ Unexpected shape {layer_attn.shape}. Skipping Layer {layer_idx + 1}.\")\n",
    "        continue\n",
    "\n",
    "    # ✅ Normalize the attention map\n",
    "    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-9)\n",
    "\n",
    "    # ✅ Ensure the attention map is 2D (reshape if needed)\n",
    "    seq_length = int(attn_map.shape[-1] ** 0.5)  # Assuming attention is square-like\n",
    "    if attn_map.shape[-1] != seq_length ** 2:\n",
    "        print(f\"⚠️ Skipping Layer {layer_idx + 1}: Non-square attention shape {attn_map.shape}\")\n",
    "        continue\n",
    "\n",
    "    attn_map = attn_map.reshape(seq_length, seq_length)  # Convert to 2D grid\n",
    "\n",
    "    # ✅ Resize attention map to match image dimensions\n",
    "    resize_transform = transforms.Resize((image.height, image.width))\n",
    "    attn_map_resized = resize_transform(torch.tensor(attn_map).unsqueeze(0).unsqueeze(0)).squeeze().numpy()\n",
    "\n",
    "    # ✅ Plot the Original Image & Attention Map\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # 🔹 Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Original Image\")\n",
    "\n",
    "    # 🔹 Attention Heatmap\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(attn_map_resized, cmap=\"jet\", alpha=0.5)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Attention Heatmap - Layer {layer_idx+1}\")\n",
    "\n",
    "    # ✅ Save each heatmap separately\n",
    "    heatmap_filename = f\"attention_layer_{layer_idx+1}.png\"\n",
    "    plt.savefig(heatmap_filename, dpi=300)\n",
    "    print(f\"✅ Saved heatmap: {heatmap_filename}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a000fc-d54b-4669-b077-9238e8813873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e15dbd-e4cb-4d98-b775-5f0479437216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
